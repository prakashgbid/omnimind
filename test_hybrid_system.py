#!/usr/bin/env python3
"""
Test OmniMind Hybrid System

Demonstrates intelligent routing between:
- 5 FREE local models (80% of queries)
- Premium models (ChatGPT 5/GPT-4, Claude Opus) for complex tasks
- Automatic cost optimization
"""

import asyncio
import os
from src.providers.intelligent_router import intelligent_router, smart_complete

print("""
üß† OmniMind Hybrid Intelligence System
=======================================

Combining:
- 5 FREE Local Models (Llama, Mistral, Phi-3, DeepSeek, Gemma)
- Premium Models (ChatGPT 5.0/GPT-4, Claude 3 Opus)
- Intelligent routing to minimize costs
""")

async def test_routing():
    """Test the intelligent routing system."""
    
    # Test cases with different complexity levels
    test_cases = [
        {
            'query': "What is 2+2?",
            'expected': 'local',
            'reason': 'Simple question - local model sufficient'
        },
        {
            'query': "Write a Python function to sort a list",
            'expected': 'local',
            'reason': 'Code task - DeepSeek Coder (local) is capable'
        },
        {
            'query': "Explain the philosophical implications of consciousness in AI",
            'expected': 'premium',
            'reason': 'Complex philosophical question - benefits from premium model'
        },
        {
            'query': "Debug this React component that has a memory leak",
            'expected': 'local',
            'reason': 'Debugging task - local models can handle'
        },
        {
            'query': "Create a production-ready authentication system with OAuth2",
            'expected': 'premium',
            'reason': 'Production code mentioned - requires premium quality'
        },
        {
            'query': "List 5 benefits of microservices",
            'expected': 'local',
            'reason': 'Simple listing task - local model sufficient'
        }
    ]
    
    print("\nüìä Testing Intelligent Routing")
    print("=" * 60)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nTest {i}: {test['query'][:50]}...")
        print(f"Expected: {test['expected']} model")
        print(f"Reason: {test['reason']}")
        
        # Analyze the task
        analysis = intelligent_router.analyze_task(test['query'])
        
        print(f"\nAnalysis:")
        print(f"  ‚Ä¢ Complexity: {analysis['complexity']}")
        print(f"  ‚Ä¢ Suggested Provider: {analysis['suggested_provider']}")
        print(f"  ‚Ä¢ Suggested Model: {analysis['suggested_model']}")
        print(f"  ‚Ä¢ Estimated Cost: ${analysis['estimated_cost']:.4f}")
        print(f"  ‚Ä¢ Reasoning: {', '.join(analysis['reasoning'][:2])}")
        
        # Actually route the query (if providers are available)
        if analysis['suggested_provider'] == 'local':
            print("  ‚úÖ Using FREE local model - no cost!")
        else:
            print(f"  üí∞ Would use premium model (${analysis['estimated_cost']:.4f})")
    
    # Show routing statistics
    print("\n" + "=" * 60)
    print("üìà Routing Statistics")
    print("=" * 60)
    
    stats = intelligent_router.get_routing_stats()
    
    print(f"\nüí∞ Cost Management:")
    print(f"  ‚Ä¢ Monthly Budget: ${stats['monthly_budget']:.2f}")
    print(f"  ‚Ä¢ Current Month Cost: ${stats['current_month_cost']:.2f}")
    print(f"  ‚Ä¢ Budget Remaining: ${stats['budget_remaining']:.2f}")
    
    print(f"\nüîå Available Providers:")
    for provider in stats['available_providers']:
        print(f"  ‚Ä¢ {provider}")
    
    if stats['recommendations']:
        print(f"\nüí° Recommendations:")
        for rec in stats['recommendations']:
            print(f"  {rec}")
    
    # Demonstrate actual completion (if local models available)
    print("\n" + "=" * 60)
    print("üéØ Live Demo with Local Model")
    print("=" * 60)
    
    try:
        # Force local model for demo
        response = await smart_complete(
            "Explain the benefits of using local LLMs",
            force_local=True
        )
        
        print(f"\nModel Used: {response.model}")
        print(f"Provider: {response.provider}")
        print(f"Cost: ${response.cost:.4f}")
        print(f"Latency: {response.latency_ms:.0f}ms")
        print(f"\nResponse: {response.content[:300]}...")
        
        if 'routing' in response.metadata:
            routing_info = response.metadata['routing']
            print(f"\nRouting Decision:")
            print(f"  ‚Ä¢ Analysis: {routing_info['analysis']['complexity']} complexity")
            print(f"  ‚Ä¢ Budget Status: ${routing_info['budget_remaining']:.2f} remaining")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not complete demo: {e}")
        print("Make sure Ollama is running for local models")

# Summary
print("\n" + "=" * 60)
print("üéØ HYBRID SYSTEM BENEFITS")
print("=" * 60)

print("""
‚úÖ Cost Optimization:
   ‚Ä¢ 80% queries use FREE local models
   ‚Ä¢ Premium models only for complex/critical tasks
   ‚Ä¢ Monthly budget: $10 (adjustable)
   ‚Ä¢ Automatic fallback to local when budget exceeded

‚úÖ Quality Assurance:
   ‚Ä¢ Critical tasks get premium models
   ‚Ä¢ Production code uses best available
   ‚Ä¢ Complex reasoning uses GPT-4 or Claude Opus
   ‚Ä¢ Simple tasks efficiently handled locally

‚úÖ Smart Routing Logic:
   ‚Ä¢ Analyzes each query for complexity
   ‚Ä¢ Checks budget before using premium
   ‚Ä¢ Falls back gracefully if premium unavailable
   ‚Ä¢ Learns from usage patterns

‚úÖ Available Models:
   LOCAL (FREE):
   ‚Ä¢ Llama 3.2 - General purpose
   ‚Ä¢ Mistral 7B - Complex reasoning
   ‚Ä¢ DeepSeek Coder - Programming
   ‚Ä¢ Phi-3 - Efficient tasks
   ‚Ä¢ Gemma 2 - Quick responses
   
   PREMIUM (When needed):
   ‚Ä¢ ChatGPT 5.0 - Most advanced (when available)
   ‚Ä¢ GPT-4 Turbo - Advanced reasoning
   ‚Ä¢ Claude 3 Opus - Nuanced understanding
   ‚Ä¢ Claude 3 Sonnet - Balanced performance
   ‚Ä¢ Claude 3 Haiku - Fast simple tasks

üí∞ Typical Monthly Cost:
   ‚Ä¢ Heavy usage: $5-10/month
   ‚Ä¢ Normal usage: $2-5/month
   ‚Ä¢ Light usage: $0-2/month
   ‚Ä¢ Local only: $0/month

üîí Privacy Options:
   ‚Ä¢ force_local=True - 100% private, no data leaves machine
   ‚Ä¢ Default - Smart mix of local and cloud
   ‚Ä¢ require_quality=True - Use best available model
""")

# Run the test
if __name__ == "__main__":
    asyncio.run(test_routing())